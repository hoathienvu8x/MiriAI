#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import warnings
warnings.filterwarnings('ignore')
import gi
gi.require_version('Gtk', '3.0')
from gi.repository import Gtk
import os
import sqlite3
import re
import itertools
import multiprocessing
import pickle
import string
import time
import unicodedata as ud
from typing import List, Iterable, Generator
from sklearn.model_selection import train_test_split
from sklearn.metrics import make_scorer
import scipy.stats
import sklearn_crfsuite
from sklearn_crfsuite import metrics
from sklearn.metrics import make_scorer
from sklearn.model_selection import RandomizedSearchCV

TAGS = ['Nc','Ny','Np','Nu','A','C','E','I','L','M','N','P','R','S','T','V','X','F']

PUNCTUATION = "@`#$%&~|[]<>'(){}*+-=;,?.!:\"/"

def has_tagged(word):
    for tag in TAGS:
        tag = '/'+tag
        if word[-len(tag):] == tag:
            return True
    return False

def normalize_text(text):
    if '/B_W' in text or '/I_W' in text:
        tokens = text.strip().split(' ')
        text = ''
        for tok in tokens:
            word_tag = tok.split('/')
            tag = word_tag.pop()
            word = '/'.join(word_tag)
            if tag == 'I_W':
                text += '_'
            else:
                text += ' '
            text += word
    for t in TAGS:
        text = text.replace('/'+t+' ',' ')
    return text.strip()

def regx(text):
    ans = text.replace("…","...")
    ans = ans.replace("“","\"")
    ans = ans.replace("”","\"")
    ans = ans.replace("–","-")
    ans = ans.replace(""," ")
    for ch in PUNCTUATION:
        ans = ans.replace(ch,' ' + ch + ' ')
    return ans

def is_word(word):
    if word in PUNCTUATION:
        return False
    if word.isdigit():
        return False
    for ch in PUNCTUATION + '1234567890':
        if ch in word:
            return False
    return True

def tokenize(text):
    ans = regx(text)
    ans += " "
    ans = ans.replace("_", " ")
    return [ w.strip() for w in ans.split(' ') if w.strip() ]

class VTrie:
    def __init__(self):
        self.next = {}
        self.is_word = False
        

    def trail_depth(self, word_gen: Generator[str, None, None]) -> int:
        depth = 0
        max_depth = depth
        tmp = self
        for token in word_gen:
            if token not in tmp.next:
                return max_depth
            tmp = tmp.next[token]
            depth += 1
            max_depth = depth if tmp.is_word else max_depth
        return max_depth

    def extract_words(self, original: str) -> List[str]:
        sentences = [sentence for sentence in re.split('[!.?,]+', original)]
        words = []
        for sentence in sentences:
            tokens = [token for token in sentence.split(" ") if token.strip() ]
            if not tokens:
                continue
            i = 0
            while i < len(tokens):
                tmp = i
                while tmp < len(tokens) and tokens[tmp][0].isupper():
                    tmp += 1
                if tmp != i:
                    words.append(" ".join(tokens[i:tmp]))
                i = tmp
                if i == len(tokens):
                    break
                word_gen = itertools.islice(tokens , i, len(tokens))
                depth = max(1, self.trail_depth(word_gen))
                words.append(" ".join(tokens[i:i+depth]))
                i += depth
        return words

    def has_word(self, word: str) -> bool:
        tokens = word.split(" ")
        tmp = self
        for token in tokens:
            if token not in tmp.next:
                return False
            tmp = tmp.next[token]
        return tmp.is_word

    def add_word(self, word: str):
        tokens = word.lower().split(" ")
        tmp = self
        for token in tokens:
            if token not in tmp.next:
                tmp.next[token] = self.__class__()
            tmp = tmp.next[token]
        tmp.is_word = True

class Tokenizer:
    bi_grams = set()
    tri_grams = set()
    model = None
    with open(os.path.join(os.path.dirname(__file__), 'models', 'vocab.txt'), 'r', encoding='utf-8') as fin:
        for token in fin.read().split('\n'):
            tmp = token.split(' ')
            if len(tmp) == 2:
                bi_grams.add(token)
            elif len(tmp) == 3:
                tri_grams.add(token)

    def __init__(self, kernel):
        self.load_kernel(kernel)

    def load_kernel(self, kernel):
        with open(kernel, 'rb') as fin:
            Tokenizer.model = pickle.load(fin)

    @staticmethod
    def gen_tag(word):
        syllables = word.split('_')
        if not any(syllables):
            return [(word, 'B_W')]
        output = [(syllables[0], 'B_W')]
        for item in syllables[1:]:
            output.append((item, 'I_W'))
        return output

    @staticmethod
    def sent2labels(sent):
        return [label for _, label in sent]

    @staticmethod
    def train(data, saveTo):
        train_data = []
        for row in data:
            tokens = row.split(' ')
            sentence = []
            for token in tokens:
                token = token.strip()
                if not token:
                    continue
                sentence.extend(Tokenizer.gen_tag(token))
            train_data.append(sentence)
        if len(train_data) == 0:
            return
        sentences = [ Tokenizer.sent2features(sent, True) for sent in train_data ]
        labels = [ Tokenizer.sent2labels(sent) for sent in train_data ]
        X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.33, random_state=42)
        params_space = {
            'c1': scipy.stats.expon(scale=0.5),
            'c2': scipy.stats.expon(scale=0.05),
        }
        crf = sklearn_crfsuite.CRF(
            algorithm='lbfgs',
            max_iterations=100,
            all_possible_transitions=True,
            c1=params_space['c1'],
            c2=params_space['c2']
        )
        f1_scorer = make_scorer(metrics.flat_f1_score,average='weighted', labels=['B_W', 'I_W'])
        rs = RandomizedSearchCV( crf, params_space,cv=5,verbose=1, n_jobs=12, n_iter=50, scoring=f1_scorer)
        rs.fit(X_train, y_train)
        print('best params:', rs.best_params_)
        print('best CV score:', rs.best_score_)
        with open(saveTo, 'wb') as pkl:
            pickle.dump(rs.best_estimator_, pkl)

    @staticmethod
    def word2features(sent, i, is_training):
        word = sent[i][0] if is_training else sent[i]
        features = {
            'bias': 1.0,
            'word.lower()': word.lower(),
            'word.isupper()': word.isupper(),
            'word.istitle()': word.istitle(),
            'word.isdigit()': word.isdigit(),
        }
        if i > 0:
            word1 = sent[i - 1][0] if is_training else sent[i - 1]
            features.update({
                '-1:word.lower()': word1.lower(),
                '-1:word.istitle()': word1.istitle(),
                '-1:word.isupper()': word1.isupper(),
                '-1:word.bi_gram()': ' '.join([word1, word]).lower() in Tokenizer.bi_grams,
            })
            if i > 1:
                word2 = sent[i - 2][0] if is_training else sent[i - 2]
                features.update({
                    '-2:word.tri_gram()': ' '.join([word2, word1, word]).lower() in Tokenizer.tri_grams,
                })
        if i < len(sent) - 1:
            word1 = sent[i + 1][0] if is_training else sent[i + 1]
            features.update({
                '+1:word.lower()': word1.lower(),
                '+1:word.istitle()': word1.istitle(),
                '+1:word.isupper()': word1.isupper(),
                '+1:word.bi_gram()': ' '.join([word, word1]).lower() in Tokenizer.bi_grams,
            })
            if i < len(sent) - 2:
                word2 = sent[i + 2][0] if is_training else sent[i + 2]
                features.update({
                    '+2:word.tri_gram()': ' '.join([word, word1, word2]).lower() in Tokenizer.tri_grams,
                })
        return features

    @staticmethod
    def sent2features(sent, is_training):
        return [Tokenizer.word2features(sent, i, is_training) for i in range(len(sent))]

    @staticmethod
    def sylabelize(text):
        text = ud.normalize('NFC', text)

        specials = ["==>", "->", "\.\.\.", ">>",'\n']
        digit = "\d+([\.,_]\d+)+"
        email = "([a-zA-Z0-9_.+-]+@([a-zA-Z0-9-]+\.)+[a-zA-Z0-9-]+)"
        web = "\w+://[^\s]+"
        word = "\w+"
        non_word = "[^\w\s]"
        abbreviations = [
            "[A-ZĐ]+\.",
            "Tp\.",
            "Mr\.", "Mrs\.", "Ms\.",
            "Dr\.", "ThS\."
        ]

        patterns = []
        patterns.extend(abbreviations)
        patterns.extend(specials)
        patterns.extend([web, email])
        patterns.extend([digit, non_word, word])

        patterns = "(" + "|".join(patterns) + ")"
        if isinstance(patterns,bytes):
            patterns = patterns.decode('utf-8')
        tokens = re.findall(patterns, text, re.UNICODE)

        return text, [token[0] for token in tokens]

    @staticmethod
    def tokenize(str):
        text, tmp = Tokenizer.sylabelize(str)
        if len(tmp) == 0:
            return str
        labels = Tokenizer.model.predict([Tokenizer.sent2features(tmp, False)])
        output = tmp[0]
        for i in range(1, len(labels[0])):
            if labels[0][i] == 'I_W' and tmp[i] not in string.punctuation and\
                            tmp[i-1] not in string.punctuation and\
                    not tmp[i][0].isdigit() and not tmp[i-1][0].isdigit()\
                    and not (tmp[i][0].istitle() and not tmp[i-1][0].istitle()):
                output = output + '_' + tmp[i]
            else:
                output = output + ' ' + tmp[i]
        return output

class Tagger:
    filtered_tags = set(string.punctuation)
    filtered_tags.add(u'\u2026')
    filtered_tags.add(u'\u201d')
    filtered_tags.add(u'\u201c')
    filtered_tags.add(u'\u2019')
    filtered_tags.add('...')
    model = None

    def __init__(self, kernel):
        self.load_kernel(kernel)

    def load_kernel(self, kernel):
        with open(kernel, 'rb') as fin:
            Tagger.model = pickle.load(fin)

    @staticmethod
    def gen_tag(word):
        if has_tagged(word):
            word_tag = word.split('/')
            tag = word_tag.pop()
            word = '/'.join(word_tag)
            return [(word, tag)]
        return [(word, 'N')]

    @staticmethod
    def sent2labels(sent):
        return [label for _, label in sent]

    @staticmethod
    def train(data, saveTo):
        train_data = []
        for row in data:
            tokens = row.split(' ')
            sentence = []
            for token in tokens:
                token = token.strip()
                if not token:
                    continue
                sentence.extend(Tagger.gen_tag(token))
            train_data.append(sentence)
        if len(train_data) == 0:
            return
        sentences = [ Tagger.sent2features(sent, True) for sent in train_data ]
        labels = [ Tagger.sent2labels(sent) for sent in train_data ]
        X_train, X_test, y_train, y_test = train_test_split(sentences, labels, test_size=0.33, random_state=42)
        params_space = {
            'c1': scipy.stats.expon(scale=0.5),
            'c2': scipy.stats.expon(scale=0.05),
        }
        crf = sklearn_crfsuite.CRF(
            algorithm='lbfgs',
            max_iterations=100,
            all_possible_transitions=True,
            c1=params_space['c1'],
            c2=params_space['c2']
        )
        f1_scorer = make_scorer(metrics.flat_f1_score,average='weighted', labels=TAGS)
        rs = RandomizedSearchCV( crf, params_space,cv=5,verbose=1, n_jobs=12, n_iter=50, scoring=f1_scorer)
        rs.fit(X_train, y_train)
        print('best params:', rs.best_params_)
        print('best CV score:', rs.best_score_)
        with open(saveTo, 'wb') as pkl:
            pickle.dump(rs.best_estimator_, pkl)

    @staticmethod
    def word2features(sent, i, is_training):
        word = sent[i][0] if is_training else sent[i]

        features = {
            'bias': 1.0,
            'word.lower()': word.lower(),
            'word.istitle()': word.istitle(),
            'word.isdigit()': word.isdigit(),
            'word[:1].isdigit()': word[:1].isdigit(),
            'word[:3].isupper()': word[:3].isupper(),
            'word.isfiltered': word in Tagger.filtered_tags,
        }
        if i > 0:
            word1 = sent[i - 1][0] if is_training else sent[i - 1]
            features.update({
                '-1:word.lower()': word1.lower(),
                '-1:word.istitle()': word1.istitle(),
                '-1:word[:1].isdigit()': word1[:1].isdigit(),
                '-1:word[:3].isupper()': word1[:3].isupper(),
            })
            if i > 1:
                word2 = sent[i - 2][0] if is_training else sent[i - 2]
                features.update({
                    '-2:word.lower()': word2.lower(),
                    '-2:word.istitle()': word2.istitle(),
                    '-2:word.isupper()': word2.isupper(),
                })
        else:
            features['BOS'] = True
        if i < len(sent) - 1:
            word1 = sent[i + 1][0] if is_training else sent[i + 1]
            features.update({
                '+1:word.lower()': word1.lower(),
                '+1:word.istitle()': word1.istitle(),
                '+1:word[:1].isdigit()': word1[:1].isdigit(),
                '+1:word.isupper()': word1.isupper(),
            })
            if i < len(sent) - 2:
                word2 = sent[i + 2][0] if is_training else sent[i + 2]
                features.update({
                    '+2:word.lower()': word2.lower(),
                    '+2:word.istitle()': word2.istitle(),
                    '+2:word.isupper()': word2.isupper(),
                })
        else:
            features['EOS'] = True

        return features

    @staticmethod
    def sent2features(sent, is_training):
        return [Tagger.word2features(sent, i, is_training) for i in range(len(sent))]

    @staticmethod
    def postagging(str):
        return Tagger.postagging_tokens(str.split(' '))

    @staticmethod
    def postagging_tokens(tokens):
        labels = Tagger.model.predict([Tagger.sent2features(tokens, False)])
        return tokens, labels[0]

VietTrie = VTrie()
with open(os.path.join(os.path.dirname(__file__),"models", "vocab.txt"), "r", encoding='utf-8') as f:
    words = f.readlines()
words = [ word.strip() for word in words if word.strip() ]
for word in words:
    VietTrie.add_word(word)

def MiriTokenize(text, kernel=None, kernel_only=False):
    if kernel and kernel_only:
        return kernel.tokenize(text)
    features = VietTrie.extract_words(text.replace('_',' '))
    for i, v in enumerate(features):
        tmp = v.split(' ')
        if tmp[0].lower() in ["ông","bà","anh","chị","em"]:
            v = ' '.join(tmp[1:])
            features[i] = v
    tokens = set([ token for token in features if ' ' in token ])
    if kernel:
        tmp = [ word.replace('_',' ') for word in kernel.tokenize(text).split(' ') if '_' in word ]
        for token in tmp:
            v = token.split(' ')
            if v[0].lower() in ["ông","bà","anh","chị","em"]:
                token = ' '.join(v[1:])
            if token not in tokens:
                tokens.add(token)
    tokens = sorted(tokens,key=len, reverse=True)
    for token in tokens:
        text = text.replace(token, token.replace(' ','_'))
    for ch in string.punctuation:
        if ch == ' ' or ch == '_':
            continue
        text = text.replace(ch, ' '+ch+' ')
    text = re.sub(r'\s+',' ',text)
    return text.strip()

def MiriTagger(text, kernel=None):
    if kernel:
        return kernel.postagging(text.strip())
    taggers = [[],[]]
    for word in text.split(' '):
        taggers[0].append(word)
        taggers[1].append('N')
    return taggers

class MiriAI(Gtk.Window):
    def __init__(self):
        super(MiriAI, self).__init__()
        self.action = "seggment"
        self.kernel_only = False
        self.tokenize = None
        self.tagger = None
        self.last_id = -1
        self.job = None
        self.entry = None
        self.is_training = False
        self.init_ui()
        self.show_all()
        self.initialDB()

    def init_ui(self):
        self.set_border_width(15)
        self.set_title("Miri AI")
        self.set_size_request(500, 250)
        self.set_position(Gtk.WindowPosition.CENTER)
        if os.path.isfile(os.path.join(os.path.dirname(__file__),'favicon.png')):
            self.set_icon_from_file('favicon.png')
        self.set_resizable(False)
        self.connect('delete_event', self.quit)

        vbox = Gtk.Box(orientation=Gtk.Orientation.VERTICAL, spacing=6)
        vbox.set_homogeneous(False)
        self.add(vbox)

        tbox = Gtk.Box(spacing=10)
        self.btnSegmention = Gtk.Button(label="Tách từ")
        self.btnSegmention.connect("clicked", self.on_button_toggled, "1")
        tbox.pack_start(self.btnSegmention, False, False, 0)

        self.btnTagger = Gtk.Button(label="Gán nhãn")
        self.btnTagger.connect("clicked", self.on_button_toggled, "2")
        tbox.pack_start(self.btnTagger, False, False, 0)

        label = Gtk.Label(label="Kiến thức máy")
        tbox.pack_start(label, False, False, 0)

        self.comboModels = Gtk.ComboBoxText()
        self.comboModels.set_entry_text_column(0)
        self.comboModels.connect("changed", self.on_model_changed)

        for md in ["Mặc định", "Bản mới"]:
            self.comboModels.append_text(md)
        self.comboModels.set_active(0)
        tbox.pack_start(self.comboModels, False, False, 0)

        self.btnHelp = Gtk.Button(label="Trợ giúp ?")
        self.btnHelp.connect('clicked', self.on_help_clicked)
        tbox.pack_end(self.btnHelp, False, False, 0)

        css_btn = Gtk.CssProvider()
        css_btn.load_from_data(b'.gtk_button { padding:5px 10px; font-family:arial,sans-serif; font-size:medium; border-radius: 0;background-image: none;background-color:white; } .gtk_actived { background-color:transparent; border:none; }')
        self.btnSegmention.get_style_context().add_provider(css_btn, Gtk.STYLE_PROVIDER_PRIORITY_APPLICATION)
        self.btnTagger.get_style_context().add_provider(css_btn, Gtk.STYLE_PROVIDER_PRIORITY_APPLICATION)
        self.btnHelp.get_style_context().add_provider(css_btn, Gtk.STYLE_PROVIDER_PRIORITY_APPLICATION)
        self.btnSegmention.get_style_context().add_class('gtk_button')
        self.btnTagger.get_style_context().add_class('gtk_button')
        self.btnHelp.get_style_context().add_class('gtk_button')
        self.btnHelp.get_style_context().add_class('gtk_actived')
        
        vbox.pack_start(tbox,False, False, 0)

        self.on_button_toggled(self.btnSegmention, "1")

        scrolledWindow = Gtk.ScrolledWindow()
        scrolledWindow.set_hexpand(True)
        scrolledWindow.set_vexpand(True)
        scrolledWindow.set_size_request(-1, 150)
        vbox.pack_start(scrolledWindow,True, True, 1)

        self.textview = Gtk.TextView()
        self.textview.set_editable(True)
        self.textview.set_wrap_mode(Gtk.WrapMode.WORD)
        scrolledWindow.add(self.textview)

        provider = Gtk.CssProvider()
        provider.load_from_data(b'.gtk_textview { padding:10px; font-family:arial,sans-serif; font-size:medium;}')
        self.textview.get_style_context().add_provider(provider, Gtk.STYLE_PROVIDER_PRIORITY_APPLICATION)
        self.textview.get_style_context().add_class('gtk_textview')

        provider = Gtk.CssProvider()
        provider.load_from_data(b'.gtk_label { padding:0; font-family:arial,sans-serif; font-size:medium; margin:0;}')

        self.lblState = Gtk.Label(label="..", xalign=0)
        self.lblState.get_style_context().add_provider(provider, Gtk.STYLE_PROVIDER_PRIORITY_APPLICATION)
        self.lblState.get_style_context().add_class('gtk_label')
        
        vbox.pack_start(self.lblState, True, True, 0)

        hbox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=5)

        self.btnAdd = Gtk.Button(label="Thêm dữ liệu")
        self.btnAdd.connect('clicked', self.on_add_clicked)
        hbox.pack_start(self.btnAdd, True, True, 0)

        self.btnPrev = Gtk.Button(label="Trước đó")
        self.btnPrev.connect('clicked', self.on_prev_clicked)
        self.btnPrev.set_sensitive(False)
        hbox.pack_start(self.btnPrev, True, True, 0)
        
        self.btnNext = Gtk.Button(label="Tiếp theo")
        self.btnNext.connect('clicked', self.on_next_clicked)
        self.btnNext.set_sensitive(True)
        hbox.pack_start(self.btnNext, True, True, 0)

        self.btnSave = Gtk.Button(label="Lưu dữ liệu")
        self.btnSave.connect('clicked', self.on_saved_clicked)
        self.btnSave.set_sensitive(True)
        hbox.pack_start(self.btnSave, True, True, 0)

        self.btnEval = Gtk.Button(label="Dự đoán")
        self.btnEval.connect('clicked', self.on_auto_clicked)
        self.btnEval.set_sensitive(True)
        hbox.pack_start(self.btnEval, True, True, 0)

        vbox.pack_start(hbox,False, False, 0)

        hbox = Gtk.Box(orientation=Gtk.Orientation.HORIZONTAL, spacing=5)

        self.btnTrainTok = Gtk.Button(label="Huấn luyện tách từ")
        self.btnTrainTok.connect('clicked', self.on_traintok_clicked)
        self.btnTrainTok.set_sensitive(True)
        hbox.pack_start(self.btnTrainTok, True, True, 0)

        self.btnTrainTag = Gtk.Button(label="Huấn luyện gắn nhãn")
        self.btnTrainTag.connect('clicked', self.on_traintag_clicked)
        self.btnTrainTag.set_sensitive(True)
        hbox.pack_start(self.btnTrainTag, True, True, 0)

        vbox.pack_start(hbox,False, False, 0)

        separator = Gtk.Separator(orientation=Gtk.Orientation.HORIZONTAL)
        separator.set_margin_top(0)
        separator.set_margin_bottom(0)
        vbox.pack_start(separator, True, False, 0)

        self.lblstatus = Gtk.Label(label="", xalign=0)
        self.lblstatus.get_style_context().add_provider(provider, Gtk.STYLE_PROVIDER_PRIORITY_APPLICATION)
        self.lblstatus.get_style_context().add_class('gtk_label')
        vbox.pack_end(self.lblstatus,True, True, 0)

    def on_model_changed(self, combo):
        tree_iter = combo.get_active_iter()
        if tree_iter is not None:
            model = combo.get_model()
            name, _ = model[tree_iter][:2]
        else:
            entry = combo.get_child()
            name = entry.get_text()
        if name.lower() == "mặc định":
            self.load_kernel("base")
            self.kernel_only = False
        else:
            self.load_kernel("")
            self.kernel_only = True

    def load_kernel(self, version):
        if version.strip():
            version = "-%s" % version
        kernel = "seggment%s.kernel" % version
        kernel_path = os.path.join(os.path.dirname(__file__), 'models', kernel)
        if os.path.isfile(kernel_path):
            if self.tokenize is None:
                self.tokenize = Tokenizer(kernel_path)
            else:
                self.tokenize.load_kernel(kernel_path)

        kernel = "tagger%s.kernel" % version
        kernel_path = os.path.join(os.path.dirname(__file__), 'models', kernel)
        if os.path.isfile(kernel_path):
            if self.tagger is None:
                self.tagger = Tagger(kernel_path)
            else:
                self.tagger.load_kernel(kernel_path)

    def thread_segment_training(self, saveTo, data):
        lock = multiprocessing.Lock()
        lock.acquire()
        self.tokenize.train(data, saveTo)
        print('Done')
        time.sleep(0.1)
        self.job = None
        lock.release()
        self.on_traintok_clicked(self.btnTrainTok)

    def thread_tagger_training(self, saveTo, data):
        lock = multiprocessing.Lock()
        lock.acquire()
        self.tagger.train(data, saveTo)
        print('Done')
        time.sleep(0.1)
        self.job = None
        lock.release()
        self.on_traintag_clicked(self.btnTrainTag)

    def on_traintok_clicked(self, button):
        if self.is_training == False:
            # self.btnTrainTag.set_label('Huấn luyện gắn nhãn')
            self.btnTrainTag.set_sensitive(False)
            button.set_label("Dừng huấn luyện")
            self.is_training = True
            try:
                cur = self.db.cursor()
                qry = cur.execute("SELECT tokens FROM sentences").fetchall()
                train_data = []
                for row in qry:
                    if row[0]:
                        train_data.append(row[0])
                self.job = multiprocessing.Process(target=self.thread_segment_training, args=(os.path.join(os.path.dirname(__file__),"models","seggment.kernel"), train_data))
                self.job.start()
            except:
                pass
        else:
            self.is_training = False
            self.btnTrainTag.set_sensitive(True)
            button.set_label('Huấn luyện tách từ')
            if self.job:
                self.job.terminate()

    def on_traintag_clicked(self, button):
        if self.is_training == False:
            # self.btnTrainTok.set_label('Huấn luyện tách từ')
            self.btnTrainTok.set_sensitive(False)
            button.set_label("Dừng huấn luyện")
            self.is_training = True
            try:
                cur = self.db.cursor()
                qry = cur.execute("SELECT tagged FROM sentences").fetchall()
                train_data = []
                for row in qry:
                    if row[0]:
                        train_data.append(row[0])
                self.job = multiprocessing.Process(target=self.thread_tagger_training, args=(os.path.join(os.path.dirname(__file__),"models","tagger.kernel"), train_data))
                self.job.start()
            except:
                pass      
        else:
            self.is_training = False
            self.btnTrainTok.set_sensitive(True)
            button.set_label("Huấn luyện gắn nhãn")
            if self.job:
                self.job.terminate()

    def on_button_toggled(self, button, name):
        if name == "1":
            self.btnSegmention.get_style_context().remove_class('gtk_actived')
            self.btnTagger.get_style_context().add_class('gtk_actived')
            self.action = "seggment"
        else:
            self.btnTagger.get_style_context().remove_class('gtk_actived')
            self.btnSegmention.get_style_context().add_class('gtk_actived')
            self.action = "tagger"
        if self.entry:
            if self.action == "seggment":
                tokens = (self.entry['tokens'] if self.entry['tokens'] else self.entry['sentence']).split(' ')
                for i, tok in enumerate(tokens):
                    if '_' in tok:
                        tokens[i] = '<b><span color="blue">'+tok+'</span></b>'
            else:
                tokens = (self.entry['tagged'] if self.entry['tagged'] else self.entry['sentence']).split(' ')
                for i, tok in enumerate(tokens):
                    if has_tagged(tok):
                        word_tag = tok.split('/')
                        tag = word_tag.pop()
                        word = '/'.join(word_tag)
                        tokens[i] = word+'<b><span color="blue">/'+tag+'</span></b>'
            text_buffer = Gtk.TextBuffer()
            self.textview.set_buffer(text_buffer)
            text_buffer.insert_markup(text_buffer.get_start_iter(),' '.join(tokens).replace('&','&amp;'), -1)

    def on_auto_clicked(self, button):
        textbuffer = self.textview.get_buffer()
        text = textbuffer.get_text(textbuffer.get_start_iter(),textbuffer.get_end_iter(),True).strip()
        if text:
            text = text.replace('”','"')
            text = text.replace('“','"')
            text = normalize_text(text+' ')
            if self.action == "seggment":
                tokens = MiriTokenize(text, self.tokenize,self.kernel_only).split(' ')
                for i, tok in enumerate(tokens):
                    if '_' in tok:
                        tok = '<b><span color="blue">'+tok+'</span></b>'
                        tokens[i] = tok
                text = ' '.join(tokens)
            else:
                if self.kernel_only:
                    result = MiriTagger(MiriTokenize(text, self.tokenize,self.kernel_only), self.tagger)
                else:
                    result = MiriTagger(text.strip(), self.tagger)
                text = ''
                for i in range(len(result[0])):
                    text += ' ' + result[0][i]+'<b><span color="blue">/'+result[1][i]+'</span></b>'
                text = text.strip()
            text_buffer = Gtk.TextBuffer()
            self.textview.set_buffer(text_buffer)
            text_buffer.insert_markup(text_buffer.get_start_iter(),text.replace('&','&amp;'), -1)
        else:
            self.lblState.set_markup('<span color="brown">Không có dữ liệu</span>')

    def on_saved_clicked(self, button):
        if self.last_id <= 0:
            self.lblState.set_markup('<span color="brown">Không có thông tin để cập nhật</span>')
            return
        textbuffer = self.textview.get_buffer()
        text = textbuffer.get_text(textbuffer.get_start_iter(),textbuffer.get_end_iter(),True).strip()
        if text:
            text = text.replace('”','"')
            text = text.replace('“','"')
            try:
                cur = self.db.cursor()
                item = cur.execute("SELECT id FROM sentences WHERE id = ? ", [ self.last_id ]).fetchone()
                if not item:
                    self.lblState.set_markup('<span color="brown">Không có thông tin để cập nhật</span>')
                    cur.close()
                    return
                act = "gắn nhãn"
                if self.action == "seggment":
                    act = "tách từ"
                    cur.execute("UPDATE sentences SET tokens = ? WHERE id = ?", [ text, self.last_id ])
                else:
                    cur.execute("UPDATE sentences SET tagged = ? WHERE id = ?", [ text, self.last_id ])
                self.db.commit()
                if self.db.total_changes >= 0:
                    if self.entry:
                        if self.action == "seggment":
                            self.entry['tokens'] = text
                        else:
                            self.entry['tagged'] = text
                    
                    self.lblState.set_markup('<span color="green">Đã cập nhật %s cho #%d</span>' % ( act, self.last_id))
                else:
                    self.lblState.set_markup('<span color="brown">Lỗi cập nhật %s cho #%d</span>' % ( act, self.last_id))
                cur.close()
            except Exception as ex:
                act = "tách từ" if self.action == "seggment" else "gắn nhãn" 
                self.lblState.set_markup('<span color="brown">Lỗi cập nhật %s: %s</span>' % (act, str(ex)))
        else:
            self.lblState.set_markup('<span color="brown">Không có dữ liệu</span>')

    def on_help_clicked(self, button):
        dialog = Gtk.MessageDialog(transient_for=self,flags=0,message_type=Gtk.MessageType.INFO,buttons=Gtk.ButtonsType.OK,text="Nhãn từ loại")
        dialog.format_secondary_text("""/A Tính từ   \t/C Liên từ   \t/E Giới từ
/I Thán từ   \t/L Định từ   \t/M Số từ
/N Danh từ   \t/Nc Phân loại   \t/Ny Viết tắt
/Np Tên riêng   \t/Nu Đơn vị   \t/P Đại từ
/R Phó từ      \t/S Yếu tố      \t/T Trợ từ
/V Động từ      \t/X Từ loại      \t/F Ký tự""")
        dialog.run()
        dialog.destroy()

    def on_add_clicked(self, button):
        textbuffer = self.textview.get_buffer()
        text = textbuffer.get_text(textbuffer.get_start_iter(),textbuffer.get_end_iter(),True).strip()
        if text:
            text = text.replace('”','"')
            text = text.replace('“','"')
            text = normalize_text(text+' ').replace('_',' ').strip()
            tokens = tokenize(text.lower())
            tokens = [ tok for tok in tokens if is_word( tok ) ]
            try:
                cur = self.db.cursor()
                item = cur.execute("SELECT id FROM sentences WHERE sentence = ?", [ text ]).fetchone()
                if item:
                    self.lblState.set_markup('<span color="brown">Đã lưu #%d</span>' % item[0])
                    self.last_id = item[0]
                    cur.close()
                    return
                cur.execute("INSERT INTO sentences (sentence, cleaned) VALUES (?, ?)", [ text, ' '.join(tokens)])
                self.db.commit()
                if self.db.total_changes > 0:
                    self.last_id = cur.lastrowid
                    self.lblState.set_markup('<span color="green">Đã lưu #%d</span>' % self.last_id)
                    cur.close()
                else:
                    cur.close()
                    self.lblState.set_markup('<span color="red">Lỗi lưu dữ liệu</span>')
            except Exception as ex:
                self.lblState.set_markup('<span color="red">Lỗi %s</span>' % str(ex))
        else:
            self.lblState.set_markup('<span color="brown">Không có dữ liệu</span>')

    def on_next_clicked(self, button):
        try:
            cur = self.db.cursor()
            item = cur.execute("SELECT id, sentence, cleaned, tokens, tagged FROM sentences WHERE id > ? ORDER BY id ASC LIMIT 1" , [ self.last_id ]).fetchone()
            if not item:
                cur.close()
                self.lblState.set_markup('<span color="brown">Không có dữ liệu</span>')
                return
            self.last_id = item[0]
            self.entry = {
                'sentence': item[1],
                'cleaned': item[2],
                'tokens': item[3],
                'tagged': item[4]
            }
            if self.action == "seggment":
                tokens = (self.entry['tokens'] if self.entry['tokens'] else self.entry['sentence']).split(' ')
                for i, tok in enumerate(tokens):
                    if '_' in tok:
                        tokens[i] = '<b><span color="blue">'+tok+'</span></b>'
            else:
                tokens = (self.entry['tagged'] if self.entry['tagged'] else self.entry['sentence']).split(' ')
                for i, tok in enumerate(tokens):
                    if has_tagged(tok):
                        word_tag = tok.split('/')
                        tag = word_tag.pop()
                        word = '/'.join(word_tag)
                        tokens[i] = word+'<b><span color="blue">/'+tag+'</span></b>'
            text_buffer = Gtk.TextBuffer()
            self.textview.set_buffer(text_buffer)
            if len(' '.join(tokens)) > 0:
                text_buffer.insert_markup(text_buffer.get_start_iter(),' '.join(tokens).replace('&','&amp;'), -1)
            item = cur.execute("SELECT id FROM sentences WHERE id < ? ORDER BY id DESC LIMIT 1" , [ self.last_id ]).fetchone()
            if item:
                self.btnPrev.set_sensitive(True)
            else:
                self.btnPrev.set_sensitive(False)
            item = cur.execute("SELECT id FROM sentences WHERE id > ? ORDER BY id DESC LIMIT 1" , [ self.last_id ]).fetchone()
            if item:
                self.btnNext.set_sensitive(True)
            else:
                self.btnNext.set_sensitive(False)
            cur.close()
        except Exception as ex:
            self.lblState.set_markup('<span color="brown">Lỗi tải dữ liệu: %s</span>' % str(ex))

    def on_prev_clicked(self, button):
        try:
            cur = self.db.cursor()
            item = cur.execute("SELECT id, sentence, cleaned, tokens, tagged FROM sentences WHERE id < ? ORDER BY id DESC LIMIT 1" , [ self.last_id ]).fetchone()
            if not item:
                cur.close()
                self.btnPrev.set_sensitive(False)
                self.lblState.set_markup('<span color="brown">Không có dữ liệu</span>')
                return
            self.last_id = item[0]
            self.entry = {
                'sentence': item[1],
                'cleaned': item[2],
                'tokens': item[3],
                'tagged': item[4]
            }
            if self.action == "seggment":
                tokens = (self.entry['tokens'] if self.entry['tokens'] else self.entry['sentence']).split(' ')
                for i, tok in enumerate(tokens):
                    if '_' in tok:
                        tokens[i] = '<b><span color="blue">'+tok+'</span></b>'
            else:
                tokens = (self.entry['tagged'] if self.entry['tagged'] else self.entry['sentence']).split(' ')
                for i, tok in enumerate(tokens):
                    if has_tagged(tok):
                        word_tag = tok.split('/')
                        tag = word_tag.pop()
                        word = '/'.join(word_tag)
                        tokens[i] = word+'<b><span color="blue">/'+tag+'</span></b>'
            text_buffer = Gtk.TextBuffer()
            self.textview.set_buffer(text_buffer)
            if len(' '.join(tokens)) > 0:
                text_buffer.insert_markup(text_buffer.get_start_iter(),' '.join(tokens).replace('&','&amp;'), -1)
            item = cur.execute("SELECT id FROM sentences WHERE id < ? ORDER BY id DESC LIMIT 1" , [ self.last_id ]).fetchone()
            if item:
                self.btnPrev.set_sensitive(True)
            else:
                self.btnPrev.set_sensitive(False)
            item = cur.execute("SELECT id FROM sentences WHERE id > ? ORDER BY id DESC LIMIT 1" , [ self.last_id ]).fetchone()
            if item:
                self.btnNext.set_sensitive(True)
            else:
                self.btnNext.set_sensitive(False)
            cur.close()
        except Exception as ex:
            self.lblState.set_markup('<span color="brown">Lỗi tải dữ liệu: %s</span>' % str(ex))

    def initialDB(self):
        try:
            self.db = sqlite3.connect(os.path.join(os.path.dirname(__file__),'miriai.db'), check_same_thread=False)
            self.db.execute("CREATE TABLE IF NOT EXISTS sentences (id INTEGER PRIMARY KEY, sentence TEXT, cleaned TEXT, tokens TEXT, tagged TEXT)")
            if self.db.total_changes >= 0:
                self.lblstatus.set_markup("<span color=\"green\">Đã kết nối tới cơ sở dữ liệu `miriai.db`</span>")
            else:
                self.lblstatus.set_markup("<span color=\"red\">Cơ sở dữ liệu `miriai.db` chưa được kết nối</span>")
        except Exception as ex:
            self.info_dialog("Lỗi kết nối tới CSDL", str(ex))

    def quit(self, sender, event):
        Gtk.main_quit()

    def info_dialog(self, text, secondary_text):
        dialogWindow = Gtk.MessageDialog(parent=self, modal=True, destroy_with_parent=True, message_type=Gtk.MessageType.INFO, buttons=Gtk.ButtonsType.OK, text=text)
        dialogWindow.format_secondary_text(secondary_text)
        dialogWindow.run()
        dialogWindow.destroy()

if __name__ == "__main__":
    app = MiriAI()
    Gtk.main()
